{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25b9180-a8f9-4284-81f0-3f493626b3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa72e93-7dcb-43e6-b48c-bf7f2d7a0897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ffe339-6538-40a9-adfd-d8706497994e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = dataset[\"train\"][\"text\"][:1000]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870aeefd-78f1-4051-b1ed-ca2fe77f0dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "W0101 22:11:45.931000 20759 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embed_model_name)\n",
    "embed_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7cbd8d-d7ed-4634-a878-3d525798ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts, tokenizer, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c70b239-6f82-4360-b596-e151df3e99c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 384])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_embeddings = embed_texts(documents, embed_tokenizer, embed_model)\n",
    "doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26247a28-7894-4f3e-a117-0aca2f47811a",
   "metadata": {},
   "source": [
    "<h3>384 is the dimensionality of the semantic embedding space.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4078f2-c581-4084-915c-5ee1cc173907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query, tokenizer, model):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            [query],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86efeb69-d21d-42fa-97e8-3af2deb01228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def semantic_search(query, documents, doc_embeddings, tokenizer, model, top_k=5):\n",
    "    query_emb = embed_query(query, tokenizer, model)\n",
    "    scores = cosine_similarity(query_emb, doc_embeddings)[0]\n",
    "    ranked_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in ranked_indices:\n",
    "        results.append((scores[idx], documents[idx]))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60cfa050-780c-4e02-9580-7e112193bb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.459 | Oil Holds Near Record Level Oil prices fell 23 cents to \\$46.35 a barrel after Venezuelan Hugo Chavez won a recall refer...\n",
      "0.433 | Stocks Fall as Oil Hits High (Reuters) Reuters - Exporters led a fall in Asian shares\\on Monday as oil prices set new hi...\n",
      "0.428 | Stocks Fall as Oil Hits High  SINGAPORE (Reuters) - Exporters led a fall in Asian shares  on Monday as oil prices set ne...\n",
      "0.409 | Oil Prices Hit Record (Reuters) Reuters - Oil prices jumped to a new record\\high near  #36;47 on Monday with traders on ...\n",
      "0.399 | No Need for OPEC to Pump More-Iran Gov  TEHRAN (Reuters) - OPEC can do nothing to douse scorching  oil prices when marke...\n"
     ]
    }
   ],
   "source": [
    "query = \"oil prices fall after global uncertainty\"\n",
    "\n",
    "results = semantic_search(\n",
    "    query,\n",
    "    documents,\n",
    "    doc_embeddings,\n",
    "    embed_tokenizer,\n",
    "    embed_model,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "for score, doc in results:\n",
    "    print(f\"{score:.3f} | {doc[:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8adff1-df5f-4417-9d74-6eb7a9a97564",
   "metadata": {},
   "source": [
    "<h1>Why embeddings + cosine similarity are better for large-scale search?</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c707ffbc-5080-42fd-a927-df06f8afa9b9",
   "metadata": {},
   "source": [
    "<h3>Because you can precompute embeddings once and search fast, instead of running the model for every queryâ€“document pair.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e43e1-f77c-4d97-9625-09887fea3abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
